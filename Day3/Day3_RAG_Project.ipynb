{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91ff393b-eebf-4e08-9755-318af2cbcf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd8e70b0-8ff9-495d-b115-3098755176e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangGraphNode:\n",
    "    rag_chain = None\n",
    "    \n",
    "    @classmethod\n",
    "    def _load(cls):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def invoke(cls, *args, **kwargs):\n",
    "        if cls.rag_chain is None:\n",
    "            cls._load()\n",
    "        return cls.rag_chain.invoke(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af43ff9-e7b4-4dbd-89cd-1a33971c1f82",
   "metadata": {},
   "source": [
    "# 1. Docs Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9591e01-0446-4312-aa04-61be28cd5b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "920c9cd0-5059-47f9-aa25-ba22e09f47ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocsRetrieval(LangGraphNode):\n",
    "    URLS = [\n",
    "        \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "        \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "        \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "    ]\n",
    "    \n",
    "    @classmethod\n",
    "    def _load(cls):\n",
    "        docs = [WebBaseLoader(url).load() for url in cls.URLS]\n",
    "        docs_list = [item for sublist in docs for item in sublist]\n",
    "        \n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "            chunk_size=250, chunk_overlap=0\n",
    "        )\n",
    "        doc_splits = text_splitter.split_documents(docs_list)\n",
    "        \n",
    "        # Add to vectorDB\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=doc_splits,\n",
    "            collection_name=\"rag-chroma\",\n",
    "            embedding=OpenAIEmbeddings(),\n",
    "        )\n",
    "        cls.rag_chain = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda07224-0fd1-445f-b5ba-0579314317c0",
   "metadata": {},
   "source": [
    "# 2. Relevance Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f92a33b7-1f84-44fe-a5dc-b553d5d19979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00eed42e-d720-4316-87cd-9e2750e87f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelevanceGrader(LangGraphNode):\n",
    "    # LLM\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    @classmethod\n",
    "    def _load(cls):\n",
    "        prompt = PromptTemplate(\n",
    "            template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance\n",
    "            of a retrieved document to a user question. If the document contains keywords related to the user question,\n",
    "            grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "            Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "            Provide the binary score as a JSON with a single key 'score' and no premable or explanation.\n",
    "             <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "            Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "            Here is the user question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "            \"\"\",\n",
    "            input_variables=[\"question\", \"document\"],\n",
    "        )\n",
    "        \n",
    "        cls.rag_chain = prompt | cls.llm | JsonOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f29717-c96e-4cbd-9953-ae1322aeac00",
   "metadata": {},
   "source": [
    "# 3. Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "749e4454-188e-412f-8298-fc8f94481cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73d45723-7825-404d-987f-f8b2d7d625bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(LangGraphNode):\n",
    "    # LLM\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    @classmethod\n",
    "    def _load(cls):\n",
    "        # Prompt\n",
    "        prompt = PromptTemplate(\n",
    "            template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an assistant for question-answering tasks.\n",
    "            Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\n",
    "            Use three sentences maximum and keep the answer concise <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "            Question: {question}\n",
    "            Context: {context}\n",
    "            Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "            input_variables=[\"question\", \"document\"],\n",
    "        )\n",
    "        \n",
    "        cls.rag_chain = prompt | cls.llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031d33f2-5840-40dd-b4e7-f8a822ea0c8a",
   "metadata": {},
   "source": [
    "# 4. Hallucination Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c7267ca-c77e-4728-aebd-db87b6244f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HallucinationGrader(LangGraphNode):\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    @classmethod\n",
    "    def _load(cls):\n",
    "        # Prompt\n",
    "        prompt = PromptTemplate(\n",
    "            template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether\n",
    "            an answer is grounded in / supported by a set of facts. Give a binary 'yes' or 'no' score to indicate\n",
    "            whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a\n",
    "            single key 'score' and no preamble or explanation. <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "            Here are the facts:\n",
    "            \\n ------- \\n\n",
    "            {documents}\n",
    "            \\n ------- \\n\n",
    "            Here is the answer: {generation}  <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "            input_variables=[\"generation\", \"documents\"],\n",
    "        )\n",
    "        \n",
    "        cls.rag_chain = prompt | cls.llm | JsonOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce8b0a5-964e-4212-996d-2130452a0a31",
   "metadata": {},
   "source": [
    "# 5. SearchEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02b4562a-8b5a-411e-80cb-1341fd774778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0cafc42-5a51-4d11-9da3-ec500ea9412c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "tavily_api_key = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2d4697e-73a1-4328-940b-3769f7716eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchEngine(LangGraphNode):\n",
    "    tavily = TavilyClient(api_key=tavily_api_key)\n",
    "\n",
    "    @classmethod\n",
    "    def _load(cls):\n",
    "        pass\n",
    "        \n",
    "    @classmethod\n",
    "    def invoke(cls, *args, **kwargs):\n",
    "        response = cls.tavily.search(*args, **kwargs)\n",
    "        context = [{\"url\": obj[\"url\"], \"title\": obj[\"title\"], \"content\": obj[\"content\"]} for obj in response['results']]\n",
    "        docs = [Document(page_content=c[\"content\"], metadata={\"url\": c[\"url\"], \"title\": c[\"title\"]}) for c in context]\n",
    "\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce76e007-c937-4f33-ba40-e0dcb3eab43c",
   "metadata": {},
   "source": [
    "# 6. Final Lang Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2cecbd4-4051-4388-851f-cb527c52779e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangGraph:\n",
    "    generation_cnt = 10\n",
    "    search_cnt = 10\n",
    "    \n",
    "    @classmethod\n",
    "    def _invoke(cls, question, docs):\n",
    "        success = False\n",
    "        answer = title = url = \"\"\n",
    "        for doc in docs:\n",
    "            doc_txt = doc.page_content\n",
    "            relevance_grade = RelevanceGrader.invoke({\"question\": question, \"document\": doc_txt})\n",
    "            if relevance_grade[\"score\"] != \"yes\":\n",
    "                continue\n",
    "                \n",
    "            success = True\n",
    "            title = doc.metadata[\"title\"]\n",
    "            if \"url\" in doc.metadata:\n",
    "                url = doc.metadata[\"url\"]\n",
    "            elif \"source\" in doc.metadata:\n",
    "                url = doc.metadata[\"source\"]\n",
    "\n",
    "            hallucinated = True\n",
    "            cnt = 0\n",
    "            while cnt < cls.generation_cnt:\n",
    "                cnt += 1\n",
    "                answer = Generator.invoke({\"question\": question, \"context\": doc_txt})\n",
    "                answer_grade = HallucinationGrader.invoke({\"generation\": answer, \"documents\": doc})\n",
    "                if answer_grade[\"score\"] == \"yes\":\n",
    "                    hallucinated = False\n",
    "                    break\n",
    "                    \n",
    "            if hallucinated:\n",
    "                answer = f\"(Hallucinated Answer){answer}\"\n",
    "\n",
    "        return success, answer, title, url\n",
    "\n",
    "    @classmethod\n",
    "    def invoke(cls, question):\n",
    "        res = \"\"\n",
    "        cnt = 0\n",
    "        while cnt < cls.search_cnt:\n",
    "            cnt += 1\n",
    "            docs = DocsRetrieval.invoke(question)\n",
    "            success, answer, title, url = cls._invoke(question, docs)\n",
    "            if not success:\n",
    "                docs = SearchEngine.invoke(question)\n",
    "                success, answer, title, url = cls._invoke(question, docs)\n",
    "            res = f\"*Answer*: {answer}\\n\\n*Url*: {url}\\n\\n*Title*: {title}\"\n",
    "            if success:\n",
    "                break\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03dd96e1-cc12-4188-948d-2bca1735d9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Answer*: Agent memory can be categorized into short-term and long-term memory. Short-term memory involves in-context learning, while long-term memory allows the agent to retain and recall information over extended periods, often using an external vector store. Additionally, agents can utilize external APIs to access information beyond their pre-trained knowledge.\n",
      "\n",
      "*Url*: https://lilianweng.github.io/posts/2023-06-23-agent/\n",
      "\n",
      "*Title*: LLM Powered Autonomous Agents | Lil'Log\n"
     ]
    }
   ],
   "source": [
    "print(LangGraph.invoke(\"agent memory\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfaf9f46-6eeb-404b-a1a5-a84631614500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Answer*: '꽁꽁 얼어붙은 한강 위로 고양이가 걸어다닙니다'는 2024년에 화제가 된 인터넷 밈으로, 2021년 12월 27일 MBN 뉴스7의 보도 장면에서 유래했습니다. 이 장면은 얼어붙은 한강 위를 걷는 고양이를 담고 있으며, 이후 다양한 패러디와 변형이 만들어졌습니다. 이 밈은 주로 고양이의 귀여움과 상황의 아이러니를 강조하는 데 사용됩니다.\n",
      "\n",
      "*Url*: https://www.etoday.co.kr/news/view/2352921\n",
      "\n",
      "*Title*: 꽁꽁 얼어붙은 한강 위로 고양이가 걸어다닙니다…뉴스밈 또 터졌다 [요즘, 이거] - 이투데이\n"
     ]
    }
   ],
   "source": [
    "print(LangGraph.invoke(\"'꽁꽁 얼어붙은 한강 위로 고양이가 걸어다닙니다' 밈에 대해서 알려줘\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ee3567-6a4f-438c-8692-a05652ff98d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992bbb25-bfb1-4f0c-bd9e-9c313e58a2bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67caaaf-a2b1-4dfa-98e8-f3bc825a9d0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e416b2-1199-43fe-9938-015df77b744a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45f1fc8-6d27-499b-9599-2638cd7292d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f19e7b7-cf79-44b5-9174-cb94b36d7c41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74249e9-fc91-4f1d-8a70-099c017117de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bd3fa6-4168-4041-9132-51015317fa49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2558467d-0593-4b63-9b4c-a08123ebae3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a17b2ce-d6af-40be-a013-d96c56a230cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bbc8af-fcf5-4d25-8d8d-fbf9c8fc571f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415af4ed-a37e-4f5a-a747-ad8ade4c8f3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cb130e-06e2-431e-aab0-7d5b93334c15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f992e620-ab32-46ea-a453-043c4c9ac1af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b6a528-5b91-48c7-bf3d-3b94ba1dced5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6ed46e-1f3f-40e8-be81-bec6ce11d7eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
